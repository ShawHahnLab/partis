#!/usr/bin/env python
from distutils.version import StrictVersion
import argparse
import copy
import time
import random
import sys
import subprocess
# import multiprocessing
import numpy
import scipy
import itertools
import collections
import traceback
if StrictVersion(scipy.__version__) < StrictVersion('0.17.0'):
    raise RuntimeError("scipy version 0.17 or later is required (found version %s)." % scipy.__version__)
import colored_traceback.always
import os
import json
partis_dir = os.path.dirname(os.path.realpath(__file__)).replace('/bin', '')
if not os.path.exists(partis_dir):
    print 'WARNING current script dir %s doesn\'t exist, so python path may not be correctly set' % partis_dir
sys.path.insert(1, partis_dir + '/python')

import utils
import glutils
import treeutils
import processargs
import seqfileopener
from partitiondriver import PartitionDriver
from clusterpath import ClusterPath
import paircluster

# ----------------------------------------------------------------------------------------
def run_simulation(args):
    if args.paired_loci:
        run_all_loci(args)
        return

    from recombinator import Recombinator

    utils.prep_dir(args.workdir)

    if args.generate_trees:  # note that if we're not simulating from scratch we kind of don't need to generate trees in a separate step (we can just set --choose-trees-in-order and heavy and light will use the same trees in the same order), but for scratch simulation they end up different (I think just from generating the germline set below). It anyway seems safer to really specify the same trees.
        import treegenerator
        tmp_shm_parameter_dir = utils.parameter_type_subdir(args, args.shm_parameter_dir) if args.shm_parameter_dir is not None else None
        treegen = treegenerator.TreeGenerator(args, tmp_shm_parameter_dir)
        treegen.generate_trees(args.seed, args.outfname, args.workdir)
        return

    default_prevalence_fname = args.workdir + '/allele-prevalence.csv'
    if args.generate_germline_set and args.allele_prevalence_fname is None:
        args.allele_prevalence_fname = default_prevalence_fname

    if args.initial_germline_dir is not None:  # the command line explicitly told us where to get the glfo
        if args.reco_parameter_dir is not None:
            print '  note: getting germline sets from --initial-germline-dir, even though --reco-parameter-dir was also set'
        input_gldir = args.initial_germline_dir
    else:  # if it wasn't explicitly set, we have to decide what to use
        if args.rearrange_from_scratch:  # just use the default
            input_gldir = args.default_initial_germline_dir
        else:  # otherwise assume we're supposed to use the glfo in --reco-parameter-dir
            input_gldir = utils.parameter_type_subdir(args, args.reco_parameter_dir) + '/' + glutils.glfo_dir
    glfo = glutils.read_glfo(input_gldir, args.locus, only_genes=args.only_genes)

    working_gldir = None  # if we generate a germline set and we're also going to run subprocesses, we need to write that glfo to disk so the subprocs can see it
    if not args.im_a_subproc and args.generate_germline_set:
        glutils.generate_germline_set(glfo, args.n_genes_per_region, args.n_sim_alleles_per_gene, args.min_sim_allele_prevalence_freq, args.allele_prevalence_fname, new_allele_info=args.new_allele_info, debug=args.debug>1)  # NOTE removes unwanted genes from <glfo>
        if utils.getsuffix(args.outfname) == '.csv':
            print '  writing generated germline set to %s/' % args.outfname.replace('.csv', '-glfo')
            glutils.write_glfo(args.outfname.replace('.csv', '-glfo'), glfo)
        if args.n_procs > 1:
            working_gldir = args.workdir + '/' + glutils.glfo_dir
            glutils.write_glfo(working_gldir, glfo)

    # ----------------------------------------------------------------------------------------
    def make_events(n_events, workdir, outfname, random_ints):
        reco = Recombinator(args, glfo, seed=args.seed, workdir=workdir)
        start = time.time()
        events = []
        for ievt in range(n_events):
            event = reco.combine(random_ints[ievt], i_choose_tree=ievt if args.choose_trees_in_order else None)
            events.append(event)
        if args.check_tree_depths or args.debug:
            reco.print_validation_values()
        utils.write_annotations(outfname, glfo, events, utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns), synth_single_seqs=utils.getsuffix(outfname) == '.csv', use_pyyaml=args.write_full_yaml_output, dont_write_git_info=args.dont_write_git_info)  # keep writing the csv as single-sequence lines, just for backwards compatibility (now trying to switch to synthesizing single seq lines when reading) NOTE list() cast is terrible, but somehow I've ended up with some of the headers as lists and some as tuples, and I can't track down all the stuff necessary to synchronize them a.t.m.
        print '    made %d event%s with %d seqs in %.1fs (%.1fs of which was running bppseqgen)' % (len(events), utils.plural(len(events)), sum(len(l['unique_ids']) for l in events), time.time()-start, sum(reco.validation_values['bpp-times']))

    if not args.im_a_subproc:
        print 'simulating'

    n_per_proc_list, work_fnames = [], []
    if args.n_procs == 1:
        make_events(args.n_sim_events, args.workdir, args.outfname, [random.randint(0, numpy.iinfo(numpy.int32).max) for _ in range(args.n_sim_events)])
    else:
        # ----------------------------------------------------------------------------------------
        def get_workdir(iproc):
            return args.workdir + '/sub-' + str(iproc)
        # ----------------------------------------------------------------------------------------
        def get_outfname(iproc):
            return get_workdir(iproc) + '/' + os.path.basename(args.outfname)
        # ----------------------------------------------------------------------------------------
        if args.input_simulation_treefname is not None:  # split up the input trees among the sub procs
            with open(args.input_simulation_treefname) as tfile:
                treelines = tfile.readlines()
            if len(treelines) < args.n_sim_events:
                print '  note: total number of trees %d less than --n-sim-events %d' % (len(treelines), args.n_sim_events)
        cmdfos = []
        for iproc in range(args.n_procs):
            n_sub_events = args.n_sim_events / args.n_procs
            if iproc == 0 and args.n_sim_events % args.n_procs > 0:  # do any extra ones in the first proc
                n_sub_events += args.n_sim_events % args.n_procs
            n_per_proc_list.append(n_sub_events)

            clist = copy.deepcopy(sys.argv)
            utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
            clist.append('--im-a-subproc')
            utils.replace_in_arglist(clist, '--seed', str(args.seed + iproc))
            utils.replace_in_arglist(clist, '--workdir', get_workdir(iproc))
            utils.replace_in_arglist(clist, '--outfname', get_outfname(iproc))
            utils.replace_in_arglist(clist, '--n-sim-events', str(n_sub_events))
            if working_gldir is not None:
                utils.replace_in_arglist(clist, '--initial-germline-dir', working_gldir)
            if args.allele_prevalence_fname is not None:
                utils.replace_in_arglist(clist, '--allele-prevalence-fname', args.allele_prevalence_fname)

            if args.input_simulation_treefname is not None:
                subtreefname = '%s/trees-sub-%d.nwk' % (args.workdir, iproc)
                utils.replace_in_arglist(clist, '--input-simulation-treefname', subtreefname)
                n_sub_trees = len(treelines) / args.n_procs
                sub_tree_lines = treelines[iproc * n_sub_trees : (iproc + 1) * n_sub_trees]
                if iproc == 0 and len(treelines) % args.n_procs > 0:  # add any extra ones to the first proc
                    sub_tree_lines += treelines[args.n_procs * n_sub_trees : ]
                if len(sub_tree_lines) == 0:
                    raise Exception('couldn\'t split up %d trees among %d procs' % (len(treelines), args.n_procs))
                if len(sub_tree_lines) < n_sub_events:
                    print '  note: number of trees %d smaller than number of events %d for sub proc %d (of %d)' % (len(sub_tree_lines), n_sub_events, iproc, args.n_procs)
                work_fnames.append(subtreefname)
                with open(subtreefname, 'w') as stfile:
                    stfile.writelines(sub_tree_lines)

            cmdstr = ' '.join(clist)
            if args.debug:
                print '  %s %s' % (utils.color('red', 'run'), cmdstr)
            cmdfos.append({'cmd_str' : cmdstr, 'workdir' : get_workdir(iproc), 'logdir' : args.workdir + '/log-' + str(iproc),'outfname' : get_outfname(iproc)})  # logdirs have to be different than <workdirs> since ./bin/partis (rightfully) barfs if its workdir already exists

        utils.run_cmds(cmdfos, batch_system=args.batch_system, batch_options=args.batch_options, batch_config_fname=args.batch_config_fname, debug='print')
        file_list = [cmdfos[i]['outfname'] for i in range(args.n_procs)]
        utils.merge_simulation_files(args.outfname, file_list, utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns),  # list() cast is terrible, but somehow I've ended up with some of the headers as lists and some as tuples, and I can't track down all the stuff necessary to synchronize them a.t.m.
                                      n_total_expected=args.n_sim_events, n_per_proc_expected=n_per_proc_list, use_pyyaml=args.write_full_yaml_output, dont_write_git_info=args.dont_write_git_info)

    if not args.im_a_subproc and args.allele_prevalence_fname is not None:  # check final prevalence freqs
        glutils.check_allele_prevalence_freqs(args.outfname, glfo, args.allele_prevalence_fname, only_region='v', debug=args.debug>1)
        if args.allele_prevalence_fname == default_prevalence_fname:
            os.remove(default_prevalence_fname)

    if working_gldir is not None:
        glutils.remove_glfo_files(working_gldir, args.locus)
    if args.n_procs > 1:
        for iproc in range(args.n_procs):
            os.rmdir(cmdfos[iproc]['logdir'])
    if not args.im_a_subproc:  # remove the dummy output file if necessary (if --outfname isn't set on the command line, we still write to a temporary output file so we can do some checks)
        if args.outfname == processargs.get_dummy_outfname(args.workdir):  # note that we *don't* want to use the light_chain arg here, that's only used in the parent process that calls the heavy and light subprocesses. In the parent process we don't want anything removed here since we still need to read both files, and in the subprocesses we want light_chain to be False
            work_fnames.append(args.outfname)
        utils.rmdir(args.workdir, fnames=work_fnames)

# ----------------------------------------------------------------------------------------
def read_inputs(args, actions):
    if actions[0] == 'cache-parameters':  # for parameter caching, use the default in data/germlines/human unless something else was set on the command line
        gldir = args.default_initial_germline_dir if args.initial_germline_dir is None else args.initial_germline_dir
    elif runs_on_existing_output(actions[0]):
        if utils.getsuffix(args.outfname) == '.csv':  # old way
            if args.initial_germline_dir is not None:
                gldir = args.initial_germline_dir
            elif args.parameter_dir is not None and os.path.exists(utils.parameter_type_subdir(args, args.parameter_dir) + '/' + glutils.glfo_dir):
                gldir = utils.parameter_type_subdir(args, args.parameter_dir) + '/' + glutils.glfo_dir
            else:
                raise Exception('couldn\'t guess germline info location with deprecated .csv output file: either set it with --intitial-germline-dir or --parameter-dir, or use .yaml output files so germline info is written to the same file as the rest of the output')
        elif utils.getsuffix(args.outfname) == '.yaml':  # new way
            gldir = None  # gets set when we read the glfo from the yaml in partitiondriver
        else:
            raise Exception('unhandled annotation file suffix %s' % args.outfname)
    else:
        if args.initial_germline_dir is not None:
            gldir = args.initial_germline_dir
        elif args.parameter_dir is not None and os.path.exists(utils.parameter_type_subdir(args, args.parameter_dir) + '/' + glutils.glfo_dir):
            gldir = utils.parameter_type_subdir(args, args.parameter_dir) + '/' + glutils.glfo_dir
        else:
            tstr = ''
            if args.parameter_dir is not None and not os.path.exists(utils.parameter_type_subdir(args, args.parameter_dir) + '/' + glutils.glfo_dir):
                tstr = ' (--parameter-dir was set to %s, but the corresponding glfo dir %s doesn\'t exist)' % (args.parameter_dir, utils.parameter_type_subdir(args, args.parameter_dir) + '/' + glutils.glfo_dir)
            raise Exception('couldn\'t guess germline info location: set it with either --intitial-germline-dir or --parameter-dir%s' % tstr)

    glfo = None
    if gldir is not None:
        template_glfo = None
        if args.sanitize_input_germlines:
            print '   using default germline dir %s for template glfo (e.g. for codon positions)' % args.default_initial_germline_dir
            template_glfo = glutils.read_glfo(args.default_initial_germline_dir, args.locus)
        glfo = glutils.read_glfo(gldir, args.locus, only_genes=args.only_genes, template_glfo=template_glfo, add_dummy_name_components=args.sanitize_input_germlines, debug=2 if args.sanitize_input_germlines else False)

    simglfo = None
    if not args.is_data:
        if args.infname is not None and utils.getsuffix(args.infname) == '.yaml':
            simglfo = None  # cause we'll read it from the input file
        elif args.simulation_germline_dir is not None:  # if an explicit dir was set on the command line
            simglfo = glutils.read_glfo(args.simulation_germline_dir, locus=args.locus)  # probably don't apply <args.only_genes> (?)
        else:
            raise Exception('couldn\'t find simulation germline info: either set with --simulation-germline-dir, or use .yaml simulation files and set --infname')

    more_input_info = None
    if args.queries_to_include_fname is not None:
        more_input_info, _, _ = seqfileopener.read_sequence_file(args.queries_to_include_fname, args.is_data)
        print '  --queries-to-include-fname: adding %d extra sequence%s from %s' % (len(more_input_info), utils.plural(len(more_input_info)), args.queries_to_include_fname)
        if args.queries_to_include is None:
            args.queries_to_include = []
        args.queries_to_include += more_input_info.keys()

    if args.infname is None:  # put this *after* setting queries_to_include from a file, since we need that set
        return None, None, glfo, simglfo

    input_info, reco_info, yaml_glfo = seqfileopener.read_sequence_file(args.infname, args.is_data, n_max_queries=args.n_max_queries, args=args, simglfo=simglfo, more_input_info=more_input_info)
    if not args.is_data and yaml_glfo is not None:  # NOTE is is extremely important that <glfo> doesn't get set to the true info in a simulation yaml
        simglfo = yaml_glfo

    if len(input_info) > 1000 and args.n_procs == 1:
        print '  note: running on %d sequences with only %d process%s. This may be kinda slow, so it might be a good idea to set --n-procs N to the number of processors on your local machine, or look into non-local parallelization with --batch-system.' % (len(input_info), args.n_procs, utils.plural(args.n_procs, prefix='e'))
    if len(input_info) > 1000 and args.outfname is None:
        print '  note: running on a lot of sequences (%d) without setting --outfname. Which is ok, but there will be no persistent record of the results (except the parameter directory).' % len(input_info)
    return input_info, reco_info, glfo, simglfo

# ----------------------------------------------------------------------------------------
def default_parameter_dir(args):
    if args.paired_loci and args.paired_indir is not None:
        instr = args.paired_indir
    elif args.infname is not None:
        instr = args.infname
    else:
        assert args.action in processargs.actions_not_requiring_input
        return 'xxx-dummy-xxx'  # this is a shitty convention, but code further on crashes if I let the parameter dir be None
    return '_output/%s' % instr[ : instr.rfind('.')].replace('/', '_')

# ----------------------------------------------------------------------------------------
def run_partitiondriver(args):
    if args.paired_loci:
        run_all_loci(args)
        return

    if args.parameter_dir is None:
        args.parameter_dir = default_parameter_dir(args)
        print '  note: --parameter-dir not set, so using default: %s' % args.parameter_dir

    actions = [args.action]  # do *not* use <args.action> after this (well, for anything other than checking what was actually set on the command line)
    if args.action in ['annotate', 'partition'] and not os.path.exists(args.parameter_dir):
        if args.refuse_to_cache_parameters:
            raise Exception('--parameter-dir %s doesn\'t exist, and --refuse-to-cache-parameters was set' % args.parameter_dir)
        actions = ['cache-parameters'] + actions
        print '  parameter dir does not exist, so caching a new set of parameters before running action \'%s\': %s' % (actions[1], args.parameter_dir)
        if args.seed_unique_id is not None or args.seed_seq is not None:  # if we're auto parameter caching for/before seed partitioning, we *don't* (yet) want to remove non-clonal sequences, since we need all the non-clonal sequences to get better parameters (maybe at some point we want to be able to count parameters just on this lineage, but for now let's keep it simple)
            raise Exception('if setting --seed-unique-id or --seed-seq for \'partition\', you must first explicitly run \'cache-parameters\' in order to ensure that parameters are cached on all sequences, not just clonally related sequences.')

    input_info, reco_info, glfo, simglfo = read_inputs(args, actions)
    parter = PartitionDriver(args, glfo, input_info, simglfo, reco_info)
    parter.run(actions)
    if not runs_on_existing_output(args.action):  # mostly wanted to avoid rewriting the persistent hmm cache file
        parter.clean()

# ----------------------------------------------------------------------------------------
# it would be really nice if this didn't need to exist, it's complicated (but it's necessitated by needing to auto-cache parameters on the full un-split igh seqs when splitting loci)
# note that i could also just stop having action-specific arguments, but oh well this seems better for now at least
def remove_action_specific_args(clist, action, debug=True):
    if action not in subargs:  # nothing to do
        return
    act_args = set(afo['name'] for afo in subargs[action])
    iarg = 0
    removed_strs = []
    while iarg < len(clist):
        if clist[iarg][:2] != '--':  # skip arg values
            iarg += 1
            continue
        if any(astr.find(clist[iarg]) == 0 for astr in act_args):  # have to allow for incompletely-written args, since argparser handles those
            removed_strs.append(clist[iarg])
            clist.pop(iarg)
            if iarg < len(clist) and clist[iarg][:2] != '--':  # if it has an arg value, also remove that
                removed_strs.append(clist[iarg])
                clist.pop(iarg)
        else:
            iarg += 1
    if debug and len(removed_strs) > 0:
        print '    removed %d arg strs that were specific to action \'%s\': %s' % (len(removed_strs), action, ' '.join(removed_strs))

# ----------------------------------------------------------------------------------------
def run_all_loci(args, ig_or_tr='ig'):
    # ----------------------------------------------------------------------------------------
    def getpdir(ltmp, bpdir=None):  # pass in <ltmp> of '' to get the base dir
        if bpdir is not None:
            return '%s/%s' % (bpdir, ltmp)
        else:
            return '%s/parameters/%s' % (utils.non_none([args.parameter_dir, args.paired_outdir, default_parameter_dir(args)]), ltmp)  # , args.workdir
    # ----------------------------------------------------------------------------------------
    def getifn(ltmp):
        if args.paired_indir is not None:
            yfn, ffn = [paircluster.paired_fn(args.paired_indir, ltmp, suffix=sx) for sx in ['.yaml', '.fa']]
            if [os.path.exists(f) for f in [yfn, ffn]].count(True) == 2:
                raise Exception('both %s and %s exist, not sure which to use' % (yfn, ffn))
            return yfn if os.path.exists(yfn) else ffn
        elif args.infname is not None:  # if we ran split-input.py on --infname
            return '%s/%s.fa' % (utils.non_none([args.paired_outdir, args.workdir]), ltmp)
        else:
            return 'XXX-DUMMY-XXX'  # ok this kind of sucks, but i think it's better than rewriting all the stuff for merge-paired-partitions (which is the one that doesn't need input stuff specified)
    # ----------------------------------------------------------------------------------------
    def getodir(lpair=None, single_chain=False, is_plotting=False):
        odir = utils.non_none([args.paired_outdir, args.workdir])
        if is_plotting and args.plotdir is not None:
            odir = args.plotdir
        if lpair is not None:
            odir = '%s/%s' % (odir, '+'.join(lpair))
        if single_chain:
            odir = '%s/single-chain' % odir
        return odir
    # ----------------------------------------------------------------------------------------
    def getplotdir(ltmp, lpair=None, single_chain=False, tmpaction=None):
        return '%s/plots%s%s' % (getodir(lpair=lpair, single_chain=single_chain, is_plotting=True), ('/'+ltmp) if ltmp is not None else '', '/parameters' if tmpaction=='cache-parameters' else '')
    # ----------------------------------------------------------------------------------------
    def getofn(ltmp, joint=False, partition_only=False, trees=False, lpair=None, input_meta=False):
        if trees:
            bname = 'trees.nwk'
        elif input_meta and args.input_metafname is not None and ltmp is None:  # if ltmp is set, we're rewriting the meta files for each locus
            return args.input_metafname
        elif args.action == 'simulate':
            return paircluster.paired_fn(getodir(), ltmp, lpair=lpair, suffix='.yaml')  # kind of weird that this getodir() call doesn't need any args
        else:
            if args.action == 'simulate':
                bstr = 'simu'
            elif input_meta:
                bstr = 'meta'
            else:
                bstr ='partition'
            bname = '%s%s.yaml' % (bstr, '-'+ltmp if ltmp is not None else '')
        ofn = '%s/%s' % (getodir(lpair=lpair, single_chain=not joint and lpair is None), bname)
        if partition_only:
            ofn = utils.insert_before_suffix('-only-partition', ofn)
        return ofn
    # ----------------------------------------------------------------------------------------
    def getmfn(ftype):  # ok this name sucks, but this fcn is for in/out files with all loci together
        return '%s/%s.%s' % (getodir(), ftype, 'fa' if 'seqs' in ftype else 'yaml')
    # ----------------------------------------------------------------------------------------
    def makelink(target, link_name):
        target = target.replace(getodir()+'/', '')
        link_name = link_name.replace(getodir()+'/', '')
        if '/' in link_name:  # it's a subdir of getodir(), we need to add at least one ../
            n_slashes = [x[0] for x in itertools.groupby(link_name)].count('/')  # have to collapse any adjacent /s
            target = n_slashes * '../' + target
        utils.simplerun('cd %s && ln -sf %s %s' % (getodir(), target, link_name), shell=True, dryrun=args.dry_run)
    # ----------------------------------------------------------------------------------------
    def prep_inputs():
        if args.infname is not None:
            cmd = './bin/extract-pairing-info.py %s %s' % (args.infname, getofn(None, joint=True, input_meta=True))
            if args.n_max_queries > 0:
                cmd += ' --n-max-queries %d' % args.n_max_queries
            try:
                utils.simplerun(cmd, dryrun=args.dry_run)
            except:
                elines = traceback.format_exception(*sys.exc_info())
                print utils.pad_lines(''.join(elines))
                print '  note: couldn\'t extract 10x-style droplet ids from sequence ids (see above), continuing without pairing info'
            cmd = './bin/split-loci.py %s --outdir %s' % (args.infname, getodir())
            if args.reverse_negative_strands:
                cmd += ' --reverse-negative-strands'
            if os.path.exists(getofn(None, joint=True, input_meta=True)):  # NOTE this won't show up up in dry_run since it won't exist yet
                cmd += ' --input-metafname %s' % getofn(None, joint=True, input_meta=True)
            if args.n_max_queries > 0:
                cmd += ' --n-max-queries %d' % args.n_max_queries
            utils.simplerun(cmd, dryrun=args.dry_run)
            if args.paired_outdir is None:
                no_pairing_info = not os.path.exists(getofn(None, joint=True, input_meta=True))
                work_fnames.extend(paircluster.paired_dir_fnames(getodir(), no_pairing_info=no_pairing_info and args.is_data, include_failed=True, include_meta=True))  # simulation input files should have pairing info (which is then overriden if an input meta file is specified)
    # ----------------------------------------------------------------------------------------
    def auto_cache_params():  # note that this somewhat duplicates code in run_partitiondriver() above (but they need to be separate because of the all-igh-seqs vs only-paired-with-igk/l thing)
        if runs_on_existing_output(args.action):  # don't need a parameter dir to merge existing paired partitions
            return False
        existing_pdir_loci = [l for l in utils.sub_loci(ig_or_tr) if os.path.exists(getpdir(l))]
        if len(existing_pdir_loci) == 0:
            if args.refuse_to_cache_parameters:
                raise Exception('parameter dirs don\'t exist, but --refuse-to-cache-parameters was set: %s' % ' '.join(getpdir(l) for l in utils.sub_loci(ig_or_tr)))
            return True  # all missing, so auto cache parameters
        elif len(existing_pdir_loci) == len(utils.sub_loci(ig_or_tr)):
            return False  # all there, don't auto-cache
        else:
            print '  %s parameters exist for some (%s) but not all (%s) loci, so not auto-caching parameters into %s' % (utils.color('yellow', 'warning'), ' '.join(existing_pdir_loci), ' '.join(set(utils.sub_loci(ig_or_tr)) - set(existing_pdir_loci)), getpdir(''))
            return False
    # ----------------------------------------------------------------------------------------
    def get_n_events(l_locus, arg_events):  # l_locus has to be set
        n_events = max(1, int(arg_events * args.light_chain_fractions[l_locus]))  # so if you don't set --n-sim-events, or if you ask for 1, you get 1 of each (i don't feel like drawing a random to see which to give you)
        if args.light_chain_fractions[l_locus] == 0:  # if you want only igk, set igl's fraction to 0
            n_events = 0
        # print '  %s+%s: %d * %.2f = %d events' % (h_locus, l_locus, arg_events, args.light_chain_fractions[l_locus], n_events)
        return n_events
    # ----------------------------------------------------------------------------------------
    def run_step(tmpaction, ltmp, auto_cache=False, skip_missing_input=False, lpair=None, joint=False):
        # ----------------------------------------------------------------------------------------
        def prep_args(ltmp):
            clist = copy.deepcopy(sys.argv)
            utils.remove_from_arglist(clist, '--paired-loci')
            utils.remove_from_arglist(clist, '--dry-run')
            utils.remove_from_arglist(clist, '--reverse-negative-strands')
            utils.remove_from_arglist(clist, '--paired-indir', has_arg=True)
            utils.remove_from_arglist(clist, '--paired-outdir', has_arg=True)
            utils.remove_from_arglist(clist, '--light-chain-fractions', has_arg=True)
            if tmpaction in ['generate-trees', 'simulate']:  # deal with simulation {,reco,shm}-parameter dir stuff
                for tstr in ['', 'reco-', 'shm-']:
                    argstr = '--%sparameter-dir' % tstr
                    if utils.is_in_arglist(clist, argstr) > 0:
                        utils.replace_in_arglist(clist, argstr, getpdir(ltmp, bpdir=utils.get_val_from_arglist(clist, argstr)))  # the trees aren't really associated with the locus if we're generating trees, but we just use it to get the mutation rate (and maybe that doesn't really get used?)  # NOTE this will use the heavy chain parameter dir and ignore the light chain one, which I think is ok (it's just used for tree scaling, and it would be better to eventually have different scaling for light chain, but it should be fine)
            if tmpaction == 'generate-trees':
                utils.remove_from_arglist(clist, '--n-procs', has_arg=True)
                utils.remove_from_arglist(clist, '--n-sim-events', has_arg=True)
                clist += ['--generate-trees']
                utils.replace_in_arglist(clist, '--outfname', getofn(ltmp, trees=True, lpair=lpair))
                work_fnames.append(getofn(ltmp, trees=True, lpair=lpair))
                utils.replace_in_arglist(clist, '--n-trees', str(get_n_events(lpair[1], utils.non_none([args.n_trees, args.n_sim_events]))), insert_after='--generate-trees')
            else:
                utils.insert_in_arglist(clist, ['--locus', ltmp], args.action)
                if args.action == 'simulate':
                    clist += ['--choose-trees-in-order', '--input-simulation-treefname', getofn(ltmp, trees=True, lpair=lpair), '--outfname', getofn(ltmp, lpair=lpair)]
                    utils.replace_in_arglist(clist, '--n-sim-events', str(get_n_events(lpair[1], args.n_sim_events)), insert_after='--choose-trees-in-order')
                else:
                    if auto_cache:
                        clist[clist.index(args.action)] = 'cache-parameters'
                        remove_action_specific_args(clist, args.action)
                    elif args.action != 'cache-parameters':
                        clist += ['--refuse-to-cache-parameters']
                    utils.replace_in_arglist(clist, '--infname', getifn(ltmp))
                    utils.replace_in_arglist(clist, '--parameter-dir', getpdir(ltmp))
                    if tmpaction != 'cache-parameters':
                        utils.insert_in_arglist(clist, ['--outfname', getofn(ltmp, joint=joint, lpair=lpair)], '--infname', has_arg=True)
                        if args.paired_outdir is None:
                            work_fnames.append(getofn(ltmp, joint=joint, lpair=lpair))
                            if not joint and lpair is None and getodir(single_chain=True) not in work_fnames:
                                work_fnames.append(getodir(single_chain=True))

            if os.path.exists(getofn(None, joint=True, input_meta=True)):  # need to add input meta file to args if we automatically generated it with extract-pairing-info.py (but this should be the one in the *parent* dir with original pairing info, whereas below in 'annotate' we want the sub-loci ones (after cleaning)
                utils.replace_in_arglist(clist, '--input-metafname', getofn(None, joint=True, lpair=lpair, input_meta=True))

            if tmpaction == 'annotate':
                clist += ['--input-partition-fname', getofn(ltmp, joint=True, partition_only=True, lpair=lpair)]
                utils.remove_from_arglist(clist, '--debug', has_arg=True)  # it crashes in utils.print_true_events() if debug is turned on, and I don't care about this debug output anyway
                if os.path.exists(getofn(None, joint=True, input_meta=True)):  # this looks for the original meta file in the parent dir, but then adds the locus-specific one, although we could use the latter for both
                    utils.replace_in_arglist(clist, '--input-metafname', getofn(ltmp, joint=True, lpair=lpair, input_meta=True))  # have to use the new, rewritten input meta file (with corrected pairing info)
                clist[clist.index(args.action)] = 'annotate'

            if args.plotdir is not None:
                utils.replace_in_arglist(clist, '--plotdir', getplotdir(ltmp, lpair=lpair, single_chain=not joint, tmpaction=tmpaction))
            if args.plot_partitions or tmpaction == 'plot-partitions':  # partitiondriver turns on partition plots if plotdir and input partition fname are both specified, so setting --plot-partitions isn't necessary... but they result in potentially different plotdirs, which is potentially important if you want plots to go somewhere specific
                if tmpaction != 'annotate':  # don't need it
                    utils.remove_from_arglist(clist, '--plot-partitions')
                if args.is_data:  # for simulation we still need it
                    utils.remove_from_arglist(clist, '--infname', has_arg=True)
                if '--plotdir' not in clist:
                    clist += ['--plotdir', getplotdir(ltmp, lpair=lpair, single_chain=not joint, tmpaction=tmpaction)]
                clist[clist.index(args.action)] = 'plot-partitions'
            if tmpaction=='get-selection-metrics' and args.is_data:  # for simulation we still need it
                utils.remove_from_arglist(clist, '--infname', has_arg=True)

            return clist
        # ----------------------------------------------------------------------------------------
        if skip_missing_input:
            if not os.path.exists(getifn(ltmp)):
                print '%s: no input file, skipping' % utils.color('blue', ltmp)
                return
            if tmpaction in ['annotate', 'partition'] and not os.path.exists(getpdir(ltmp)):  # this probably means that no sequences had annotations (i.e. all failed) for this locus when parameter caching
                print '%s: no parameter dir, skipping' % utils.color('blue', ltmp)
                return
        utils.simplerun(' '.join(prep_args(ltmp)), dryrun=args.dry_run)
    # ----------------------------------------------------------------------------------------
    def rewrite_input_metafo(ltmp, lpair, joint_partition, antn_dict):  # replace old paired uids with new, fixed ones
        old_metafos = utils.read_json_yaml(getofn(None, joint=True, input_meta=True))  # NOTE the underlying dicts get modified
        new_metafos = {}
        single_pids = {u : pids for l in antn_dict.values() for u, pids in zip(l['unique_ids'], l['paired-uids'])}
        for jclust in joint_partition:
            for uid in jclust:
                new_metafos[uid] = old_metafos.get(uid, {})  # not sure it's possible for it to be missing, but maybe
                new_metafos[uid]['paired-uids'] = single_pids.get(uid)  # if it's not in there, we couldn't decide who it was paired with
        if not os.path.exists(getodir(lpair=lpair)):
            os.makedirs(getodir(lpair=lpair))
        with open(getofn(ltmp, joint=True, lpair=lpair, input_meta=True), 'w') as mfile:
            json.dump(new_metafos, mfile)
        work_fnames.append(getofn(ltmp, joint=True, lpair=lpair, input_meta=True))
    # ----------------------------------------------------------------------------------------
    def write_joint_locus(ltmp, lpair, joint_partition, antn_dict, glfo):
        if os.path.exists(getofn(None, joint=True, input_meta=True)):
            rewrite_input_metafo(ltmp, lpair, joint_partition, antn_dict)
        missing_clusters = []
        for jclust in joint_partition:
            if ':'.join(jclust) not in antn_dict:
                missing_clusters.append(jclust)
                # overlap_clusters = [l['unique_ids'] for l in annotation_list if len(set(l['unique_ids']) & set(jclust)) > 0]
                # print '      %3d: %s' % (len(jclust), ' '.join('%3d/%3d'%(len(set(c) & set(jclust)), len(c)) for c in overlap_clusters))
        if len(missing_clusters) > 0:  # we also get the annotations for the non-missing ones, which should end up the same so it's a bit of a waste, but it's nice to not have to re-read the output file and rewrite it with the non-missing ones
            print '  need to get annotations for %d/%d joint %s clusters' % (len(missing_clusters), len(joint_partition), ltmp)
        partition_lines = ClusterPath(partition=joint_partition).get_partition_lines(True)  # we could get annotations only for the missing clusters, but then we'd have to read the resulting output files, add in the non-missing cluster annotations, and then rewrite them.
        headers = utils.add_lists(list(utils.annotation_headers), args.extra_annotation_columns)
        if not os.path.exists(getpdir(ltmp)):  # no parameter dir, so can't get new annotations
            print '  %s no %s --parameter-dir was set (or it doesn\'t exist), so we can\'t get annotations for the new joint partition (the joint partition, and the non-joint/non-merged annotations, are still in the output dir)' % (utils.color('yellow', 'warning'), ltmp)
            # TODO maybe just subset the existing annotations according to the joint partition?
            utils.write_annotations(getofn(ltmp, joint=True), glfo, [], headers, partition_lines=partition_lines)  # only actually write the partition
        else:
            utils.write_annotations(getofn(ltmp, joint=True, partition_only=True, lpair=lpair), glfo, [], headers, partition_lines=partition_lines)  # write just the partition (no annotations) to a tmp file for use by the annotation step below
            work_fnames.append(getofn(ltmp, joint=True, partition_only=True, lpair=lpair))
            run_step('annotate', ltmp, skip_missing_input=True, lpair=lpair, joint=True)  # this runs partis 'annotate' (it would be nice if i could somehow use partitiondriver.get_cluster_annotations() here, but i don't think it really makes sense/is workable)

    # ----------------------------------------------------------------------------------------
    def read_output_files(tmploci, lpair=None, debug=False):
        glfos, antn_lists, cpaths = {}, {}, {}
        for ltmp in tmploci:  # read single-locus output files
            if not os.path.exists(getofn(ltmp, lpair=lpair)):
                print '%s: no %s %s output file, skipping: %s' % (utils.color('blue', '+'.join(lpair) if lpair is not None else ltmp), ltmp, 'simulation' if args.action=='simulate' else 'partition', getofn(ltmp, lpair=lpair))
                continue
            glfos[ltmp], antn_lists[ltmp], cpaths[ltmp] = utils.read_output(getofn(ltmp, lpair=lpair), dont_add_implicit_info=not debug, skip_failed_queries=True)
        return glfos, antn_lists, cpaths
    # ----------------------------------------------------------------------------------------
    def combine_simu_pair(lpair, headers):
        glfos, antn_lists, cpaths = read_output_files(lpair, lpair=lpair, debug=args.debug)
        if any(l not in glfos for l in lpair):
            return {'glfos' : None, 'antn_lists' : None, 'cpaths' : None}
        print '%s: synchronizing heavy and light chain simulation trees and rewriting output files in %s/' % (utils.color('blue', '+'.join(lpair)), getodir(lpair=lpair))
        assert len(set(len(antn_lists[l]) for l in lpair)) == 1  # make sure the lists for both loci are the same length
        for hline, lline in zip(*[antn_lists[l] for l in lpair]):
            treeutils.merge_heavy_light_trees(hline, lline, use_identical_uids=False)
        for ltmp in lpair:  # NOTE this rewrites the files we just read, which isn't really great in terms of resiliency to crashing partway through things
            utils.write_annotations(getofn(ltmp, lpair=lpair), glfos[ltmp], antn_lists[ltmp], headers, use_pyyaml=args.write_full_yaml_output)
        return {'glfos' : glfos, 'antn_lists' : antn_lists, 'cpaths' : cpaths}
    # ----------------------------------------------------------------------------------------
    def write_merged_simu(antn_lists):  # NOTE that this writes a new input meta info file, which is where partis will then get the paird uid info if --input-metfname is set, but does *not* modify the 'paired-uids' key in the original simulation files
        outfos, metafos = [], {}
        for ltmp in antn_lists:
            for tline in antn_lists[ltmp]:
                for uid, seq, pids in zip(tline['unique_ids'], tline['input_seqs'], tline['paired-uids']):
                    outfos.append({'name' : uid, 'seq' : seq})
                    metafos[uid] = {'locus' : ltmp, 'paired-uids' : pids}

        n_droplets = int(0.5 * float(len(outfos)) / args.mean_cells_per_droplet)  # 0.5 is because <outfos> includes both heavy and light sequences
        droplet_ids = [[] for _ in range(n_droplets)]
        sfo_dict = {s['name'] : s for s in outfos}
        while len(sfo_dict) > 0:
            tid = next(iter(sfo_dict))
            idrop = numpy.random.choice(range(len(droplet_ids)))
            droplet_ids[idrop] += [tid] + metafos[tid]['paired-uids']  # add <tid> plus its paired ids to this drop
            for uid in [tid] + metafos[tid]['paired-uids']:
                sfo_dict[uid]['droplet-ids'] = droplet_ids[idrop]
                del sfo_dict[uid]
        for sfo in outfos:
            metafos[sfo['name']]['paired-uids'] = sfo['droplet-ids']

        with open(getmfn('all-seqs'), 'w') as outfile:
            for sfo in outfos:
                outfile.write('>%s\n%s\n' % (sfo['name'], sfo['seq']))
        with open(getmfn('meta'), 'w') as mfile:
            json.dump(metafos, mfile)
    # ----------------------------------------------------------------------------------------
    def concat_heavy_chain(lp_infos, write=False):  # concatenate partitions+annotations for the heavy chains paired with k and l (returns info also for the light chains, but they're unchanged)
        glfos, antn_lists, joint_cpaths = {}, {}, {}
        for lpair in utils.locus_pairs[ig_or_tr]:
            lpk = tuple(lpair)
            if lp_infos[lpk]['antn_lists'] is None:
                continue
            for ltmp in lpair:
                if ltmp in glfos:  # merge heavy chain glfos from those paired with igk and with igl
                    glfos[ltmp] = glutils.get_merged_glfo(glfos[ltmp], lp_infos[lpk]['glfos'][ltmp])
                    antn_lists[ltmp] += lp_infos[lpk]['antn_lists'][ltmp]
                    assert len(joint_cpaths[ltmp].partitions) == 1
                    joint_cpaths[ltmp] = ClusterPath(partition=joint_cpaths[ltmp].best() + lp_infos[lpk]['cpaths'][ltmp].best())
                else:
                    glfos[ltmp] = lp_infos[lpk]['glfos'][ltmp]
                    antn_lists[ltmp] = lp_infos[lpk]['antn_lists'][ltmp]
                    joint_cpaths[ltmp] = lp_infos[lpk]['cpaths'][ltmp]
        # have to remove duplicates from heavy partition and annotations (since seqs that we don't have good pairing info for get put in both light chain dirs)
        hloc = utils.get_single_entry([l for l in utils.sub_loci(ig_or_tr) if utils.has_d_gene(l)])  # very convoluted way to get heavy chain
        joint_cpaths[hloc] = ClusterPath(partition=utils.get_deduplicated_partitions([joint_cpaths[hloc].best()], antn_list=antn_lists[hloc], glfo=glfos[hloc], debug=True)[0])  # NOTE this is a pretty arbitrary way to combine the partitions for the seqs with uncertain pairing info, but whatever
        if write:
            for ltmp in utils.sub_loci(ig_or_tr):
                if utils.has_d_gene(ltmp):
                    headers = utils.add_lists(list(utils.annotation_headers), args.extra_annotation_columns)
                    utils.write_annotations(getofn(ltmp, joint=True), glfos[ltmp], antn_lists[ltmp], headers, use_pyyaml=args.write_full_yaml_output)
                else:
                    makelink(getofn(ltmp, lpair=utils.getlpair(ltmp)), getofn(ltmp, joint=True))
                if args.paired_outdir is None:
                    work_fnames.append(getofn(ltmp, joint=True))
        return glfos, antn_lists, joint_cpaths
    # ----------------------------------------------------------------------------------------
    def combine_simu_chains():
        headers = utils.add_lists(list(utils.simulation_headers), args.extra_annotation_columns)
        lp_infos = {}
        for lpair in utils.locus_pairs[ig_or_tr]:
            lp_infos[tuple(lpair)] = combine_simu_pair(lpair, headers)
        glfos, antn_lists, cpaths = concat_heavy_chain(lp_infos)
        for ltmp in [l for l in utils.sub_loci(ig_or_tr) if l in antn_lists]:
            print '  %s: writing %d annotation%s to %s' % (ltmp, len(antn_lists[ltmp]), utils.plural(len(antn_lists[ltmp])), getofn(ltmp))
            utils.write_annotations(getofn(ltmp), glfos[ltmp], antn_lists[ltmp], headers, use_pyyaml=args.write_full_yaml_output)
        if args.mean_cells_per_droplet is not None:  # NOTE this also writes a .fa with all three loci together, which should really probably be separate from whether mean_cells_per_droplet is set
            write_merged_simu(antn_lists)
    # ----------------------------------------------------------------------------------------
    def get_true_partitions(cpaths, debug=False):
        if args.is_data or args.paired_indir is None:
            return None
        true_partitions = {}
        for ltmp in utils.sub_loci(ig_or_tr):
            true_partitions[ltmp] = utils.read_cpath(getifn(ltmp), n_max_queries=args.n_max_queries).best()  # would be nice to use read_inputs(), but i think it's not worth handling changing args.locus and whatnot
        return true_partitions
    # ----------------------------------------------------------------------------------------
    def combine_inf_chains():
        DBG = True #False
        single_glfos, single_antn_lists, single_cpaths = read_output_files(utils.sub_loci(ig_or_tr), debug=DBG)  # single chain cpaths (and other info)
        if not any('paired-uids' in l for alist in single_antn_lists.values() for l in alist):
            print '  no paired uids in any annotations, not merging single chain partitions'
            return
        true_partitions = get_true_partitions(single_cpaths, debug=DBG)
        paircluster.clean_pair_info(single_cpaths, single_antn_lists, is_data=args.is_data, plotdir=getplotdir(None) if args.plotdir is not None else None, debug=DBG) #, max_hdist=4
        lp_infos = {}
        for lpair in utils.locus_pairs[ig_or_tr]:
            print '%s: synchronizing heavy and light chain cluster paths' % utils.color('blue', '+'.join(lpair))
            ploci = {ch : l for ch, l in zip(utils.chains, lpair)}
            lp_cpaths, lp_antn_lists, unpaired_seqs = paircluster.remove_badly_paired_seqs(ploci, single_cpaths, single_antn_lists, single_glfos, debug=DBG)  # NOTE that if we re-annotated after doing this, we'd in general get different annotations (i.e. this is really the first step in paired clustering, while the rest of the steps happen in the next fcn call)
            joint_partitions = paircluster.merge_chains(ploci, lp_cpaths, lp_antn_lists, unpaired_seqs=unpaired_seqs, check_partitions=True, true_partitions=true_partitions, input_cpaths=single_cpaths, input_antn_lists=single_antn_lists, debug=DBG)  # , iparts={'igl' : 7}
            for ltmp in lpair:
                write_joint_locus(ltmp, lpair, joint_partitions[ltmp], utils.get_annotation_dict(lp_antn_lists[ltmp]), single_glfos[ltmp])
            lp_infos[tuple(lpair)] = {k : v for k, v in zip(['glfos', 'antn_lists', 'cpaths'], read_output_files(lpair, lpair=lpair))}  # ick this is ugly
        _, concat_antn_lists, concat_cpaths = concat_heavy_chain(lp_infos, write=True)
        # TODO figure out about adding/removing missing uids
        ccfs = {}
        for ltmp in utils.sub_loci(ig_or_tr):
            ccfs[ltmp] = paircluster.compare_partition_pair(single_cpaths[ltmp].best(), concat_cpaths[ltmp].best(), dbg_str='%s single vs joint concat\'d '%utils.locstr(ltmp), cf_label='single', ref_label='joint', debug=True) # antn_list=XXX
        return ccfs
    # ----------------------------------------------------------------------------------------
    def run_cf_plotdirs(cfpdir, ltmp, ccfs=None):
        subd = 'partitions/sizes'
        titlestr = ltmp
        if ccfs is not None:
            titlestr += ':@specif.%.2f'%ccfs[ltmp][0]
        cmdstr = './bin/compare-plotdirs.py --outdir %s/%s --names single:joint --plotdirs %s/%s:%s/%s' % (cfpdir, ltmp, getplotdir(ltmp, single_chain=True), subd, getplotdir(ltmp), subd)
        cmdstr += ' --make-parent-html --add-to-title %s --log xy --translegend=0.07:0.07' % titlestr
        utils.simplerun(cmdstr)

        # NOTE this has to happen *after* running compare-plotdirs.py, since it cleans the dirs
        import plotting
        plotting.plot_csim_matrix_from_files(cfpdir + '/' + ltmp, 'csim-matrix', 'single partition', getofn(ltmp, joint=False), 'joint partition', getofn(ltmp, joint=True), 35)

    # ----------------------------------------------------------------------------------------
    if args.paired_outdir is None:
        print '  note: --paired-outdir is not set, so there will be no persistent record of the results (except the parameter directory).'
    utils.prep_dir(args.workdir)  # NOTE it's kind of weird to have workdir and paired_outdir, but workdir is for stuff we throw out by default, whereas paired_outdir is for stuff we save
    work_fnames = []
    prep_inputs()

    if args.action == 'simulate':
        paircluster.prep_paired_dir(getodir(), clean=True, suffix='.yaml', extra_files=[getofn(None, trees=True, lpair=lp) for lp in utils.locus_pairs[ig_or_tr]])
        for lpair in utils.locus_pairs[ig_or_tr]:
            h_locus, l_locus = lpair
            print '%s: simulating %d event%s' % (utils.color('blue', '+'.join(lpair)), get_n_events(l_locus, args.n_sim_events), utils.plural(get_n_events(l_locus, args.n_sim_events)))
            if get_n_events(l_locus, args.n_sim_events) == 0:
                continue
            run_step('generate-trees', h_locus, lpair=lpair)
            for ltmp in lpair:
                run_step(args.action, ltmp, lpair=lpair)
        if not args.dry_run:
            combine_simu_chains()
        if args.paired_outdir is None:
            work_fnames.extend(paircluster.paired_dir_fnames(getodir(), suffix='.yaml'))  # using extend() rather than += in a few places because of local variable/scoping behavior
    else:
        if args.action != 'cache-parameters' and auto_cache_params():  # ick. we can't just let the normal auto-parameter caching happen within each locus pair below, since it'll infer separately on the igh seqs paired with igk vs igl
            missing_pdir_loci = [l for l in utils.sub_loci(ig_or_tr) if not os.path.exists(getpdir(l))]
            print '  missing %d locus parameter dirs (%s), so caching a new set of parameters before running action \'%s\':  %s' % (len(missing_pdir_loci), ' '.join(missing_pdir_loci), args.action, ' '.join(getpdir(l) for l in missing_pdir_loci))
            for ltmp in utils.sub_loci(ig_or_tr):
                run_step('cache-parameters', ltmp, auto_cache=True, skip_missing_input=True)
        if args.action not in ['merge-paired-partitions', 'get-selection-metrics']:
            for ltmp in utils.sub_loci(ig_or_tr):
                run_step(args.action, ltmp, skip_missing_input=args.action != 'plot-partitions')
        ccfs = None  # if we're just plotting partitions (not merging), we'd have to go back and read single + joint/concat'd partitions in order to get these
        if args.action in ['partition', 'merge-paired-partitions'] and not args.dry_run:
            ccfs = combine_inf_chains()
        if args.action in ['get-selection-metrics', 'plot-partitions'] or args.plotdir is not None or args.plot_partitions:
            aigsm = args.action=='get-selection-metrics'
            tmpaction = 'get-selection-metrics' if aigsm else 'plot-partitions'  # NOTE there's no way to make this get selection metrics and plot partitions in one go (i.e. --get-selection-metrics won't do anything), but whatever just run it as a separate step
            for lpair in utils.locus_pairs[ig_or_tr]:  # plots for each locus in each paired (e.g. igh+igk/) subdir (single-chain plots for each locus are run above)
                for ltmp in lpair:
                    run_step(tmpaction, ltmp, lpair=lpair, joint=True)  # this runs partis 'annotate' (it would be nice if i could somehow use partitiondriver.get_cluster_annotations() here, but i don't think it really makes sense/is workable)
            cfpdir = '%s/cf-plots' % getodir(is_plotting=True)
            for ltmp in utils.sub_loci(ig_or_tr):  # heavy chain plots with the joint partition combining igh paired with both light loci (make light chain links just so people know where to find them/consistency)
                if utils.has_d_gene(ltmp):  # light chain in the paired subdirs is the same, so just link the results
                    run_step(tmpaction, ltmp, joint=True)
                else:
                    makelink(getplotdir(ltmp, lpair=utils.getlpair(ltmp)), getplotdir(ltmp))
                if not aigsm and args.plotdir is not None:
                    run_cf_plotdirs(cfpdir, ltmp, ccfs=ccfs)
            if not aigsm and args.plotdir is not None:
                fnoutstr, _ = utils.simplerun('find %s -type f -name *.svg' % cfpdir, return_out_err=True)
                import plotting
                fnlist = sorted(fnoutstr.strip().split('\n'))
                plotting.make_html(cfpdir, fnames=[[f for f in fnlist if 'cluster-sizes' in f], [f for f in fnlist if 'cluster-sizes' not in f]])

    if not args.dry_run:  # this will crash if you set plotdir, but not paired outdir, but who cares
        utils.rmdir(args.workdir, fnames=work_fnames)

# ----------------------------------------------------------------------------------------
class MultiplyInheritedFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
    pass
formatter_class = MultiplyInheritedFormatter
parser = argparse.ArgumentParser(formatter_class=MultiplyInheritedFormatter)
subparsers = parser.add_subparsers(dest='action')

parent_parser = argparse.ArgumentParser(add_help=False)

parent_parser.add_argument('--locus', default='igh', choices=utils.loci.keys(), help='which immunoglobulin or t-cell receptor locus?')
parent_parser.add_argument('--paired-loci', action='store_true', help='Set this if input contains sequences from more than one locus (igh+igk+igl all together). Input can be specified either with --infname (in which case it will be automatically split apart by loci), or with --paired-indir (whose files must conform to the same conventions). It will then run the specified action on each of the single locus input files, and (if specified) merge the resulting partitions.')
parent_parser.add_argument('--reverse-negative-strands', action='store_true', help='If --paired-loci is set, align every sequence both forwards and revcomp\'d, then for each sequence keep the sense with better alignment. If *not* running with --paired-loci, then first run bin/split-loci.py separately with --reverse-negative-strands.')
parent_parser.add_argument('--dry-run', action='store_true', help='Just print subprocess commands that would be run without actually running them (only implemented for --paired-loci).')
parent_parser.add_argument('--species', default='human', choices=('human', 'macaque', 'mouse'), help='Which species?')
parent_parser.add_argument('--queries', help='Colon-separated list of query names to which to restrict the analysis')
parent_parser.add_argument('--queries-to-include', help='When reading the input file, look for and include these additional uids when --n-random-queries is set (*not* compatible with --n-max-queries). Contrast with --queries, which includes *only* the indicated uids. Additionally, these queries are treated differently e.g. when partition plotting or removing duplicate sequences.')
parent_parser.add_argument('--queries-to-include-fname', help='In cases where you want certain sequences to be included in --queries-to-include or --seed-unique-id, but these sequences are not in --infname, you can put them in this file. Typically, this is useful when you have a number of seed sequences that are from a separate experiment than the NGS data in --infname.')
parent_parser.add_argument('--reco-ids', help='Colon-separated list of rearrangement-event IDs to which we restrict ourselves')  # or recombination events
parent_parser.add_argument('--n-max-queries', type=int, default=-1, help='Maximum number of query sequences to read from input file, starting from beginning of file')
parent_parser.add_argument('--n-random-queries', type=int, help='choose this many queries at random from entire input file')
parent_parser.add_argument('--istartstop', help='colon-separated start:stop line indices for input sequence file (with python slice conventions, e.g. if set to \'2:4\' will skip the zeroth and first sequences, and then take the following two sequences, and then skip all subsequence sequences). Applied before any other input filters, e.g. --n-max-queries, --queries, --reco-ids, etc.')

parent_parser.add_argument('--debug', type=int, default=0, choices=[0, 1, 2], help='Debug verbosity level.')
parent_parser.add_argument('--sw-debug', type=int, choices=[0, 1, 2], help='Debug level for Smith-Waterman.')
parent_parser.add_argument('--abbreviate', action='store_true', help='Abbreviate/translate sequence ids to improve readability of partition debug output. Uses a, b, c, ..., aa, ab, ...')
parent_parser.add_argument('--print-git-commit', action='store_true', help='print sys.argv, git commit hash, and tag info')
parent_parser.add_argument('--dont-write-git-info', action='store_true', help='Don\'t write git tag/commit info to yaml output files (used to enable diffing of test results)')
parent_parser.add_argument('--only-print-best-partition', action='store_true', help='when printing cluster annotations (either with --debug 1 for \'partition\', or for \'view-output\'), instead of the default of printing the annotation for every cluster for which we calculated one (see --calculate-alternative-naive-seqs and --n-partitions-to-write), only print annotations for clusters in the best partition. UPDATE also restricts to the best partition when calculating selection metrics.')
parent_parser.add_argument('--only-print-seed-clusters', action='store_true', help='same as --only-print-best-partition, but in addition, only print the seed cluster(s). Note that if --only-print-best-partition is *not* set, then there will be more than one seed cluster.')
parent_parser.add_argument('--only-print-queries-to-include-clusters', action='store_true', help='same as --only-print-best-partition, but in addition, only print the cluster(s) corresponding to --queries-to-include/--queries-to-include-fname. Note that if --only-print-best-partition is *not* set, then there will be more than one cluster for each such query.')  # NOTE we could just assume this if queries to include are specified, but that's not the behavior when we're plotting (in which case we include everybody but highlight the queries to include)

parent_parser.add_argument('--n-procs', type=int, default=1, help='Number of processes over which to parallelize. This is usually the maximum that will be initialized at any given time, but for internal reasons, certain steps (e.g. smith waterman and partition naive sequence precaching) sometimes use slightly more.')
parent_parser.add_argument('--n-max-to-calc-per-process', default=250, help='if a bcrham process calc\'d more than this many fwd + vtb values (and this is the first time with this number of procs), don\'t decrease the number of processes in the next step (default %(default)d)')
parent_parser.add_argument('--min-hmm-step-time', default=2., help='if a clustering step takes fewer than this many seconds, always reduce n_procs')
parent_parser.add_argument('--batch-system', choices=['slurm', 'sge'], help='batch system with which to attempt paralellization')
parent_parser.add_argument('--batch-options', help='additional options to apply to --batch-system (e.g. --batch-options="--foo bar")')
parent_parser.add_argument('--batch-config-fname', default='/etc/slurm-llnl/slurm.conf', help='system-wide batch system configuration file name')  # for when you're running the whole thing within one slurm allocation, i.e. with  % salloc --nodes N ./bin/partis [...]

parent_parser.add_argument('--only-smith-waterman', action='store_true', help='Exit after finishing smith-waterman.')
parent_parser.add_argument('--count-parameters', action='store_true', help='force parameter counting when action is not cache-parameters (presumably so that you can plot them, or to get multi-hmm parameters from partitioning)')
parent_parser.add_argument('--dont-write-parameters', action='store_true', help='don\'t write parameters to disk even if you\'ve counted them (mostly for use in conjunction with --only-smith-waterman, so you can avoid cluttering up your file system)')
parent_parser.add_argument('--write-trimmed-and-padded-seqs-to-sw-cachefname', action='store_true', help='after running sw, trim and pad sequences *before* writing to the sw cache file (rather than after). Note that this will in some cases cause waterer.py to not be able to read sw results from this cache file.')
parent_parser.add_argument('--partis-dir', default=partis_dir, help='for internal use only')
parent_parser.add_argument('--ig-sw-binary', default=partis_dir + '/packages/ig-sw/src/ig_align/ig-sw', help='Path to ig-sw executable.')
parent_parser.add_argument('--vsearch-binary',  help='Path to vsearch binary (vsearch binaries for linux and darwin are pre-installed in bin/, but for other systems you need to get your own)')
parent_parser.add_argument('--is-simu', action='store_true', help='Set if running on simulated sequences')
parent_parser.add_argument('--skip-unproductive', action='store_true', help='Skip sequences which Smith-Waterman determines to be unproductive (i.e. if they have stop codons, out of frame cdr3, or mutated cyst/tryp/phen)')
parent_parser.add_argument('--collapse-duplicate-sequences', action='store_true', help='Collapse all sequences that are identical from 5\' end of v to 3\' end of j under a single uid. The duplicate uids then appear in the output key \'duplicates\'. This collapse happens after any framework insertion/constant region trimming (see --dont-remove-framework-insertions).')
parent_parser.add_argument('--also-remove-duplicate-sequences-with-different-lengths', action='store_true', help='If --collapse-duplicate-sequences is set, by default we collapse together only queries that have exactly the same (coding/vdj) sequence. If this argument is also set, we also collapse sequences that are sub/super strings of each other (we keep/index by the longest one). So, e.g., this would also collapse sequences that code for the same bcr but have different read lengths/coverage. NOTE this is not the same as collapsing all sequences that are identical in the parts that they share, which would be much more computationally expensive (e.g. if one is missing start of V, the other end of J).')
parent_parser.add_argument('--dont-remove-framework-insertions', action='store_true', help='By default we trim anything to the 5\' of V and 3\' of J, since partis ignores these regions and it simplifies the output. This argument turns that off. If you want to preserve constant region information, either set this argument (if you want access to the entire constant region sequences, in \'fv_insertion\' and \'jf_insertion\'), or pass in constant region annotations with the \'constant-region\' key in --input-metafname.')
parent_parser.add_argument('--dont-rescale-emissions', action='store_true', help='Don\'t scale each hmm\'s emission probabilities to account for the branch length of each individual sequence.')
parent_parser.add_argument('--no-indels', action='store_true', help='Tell smith-waterman not to look for indels, by drastically increasing the gap-open penalty (you can also set the penalty directly).')
parent_parser.add_argument('--no-indel-gap-open-penalty', type=int, default=1000, help='set --gap-open-penalty to this when --no-indels is set (also used in python/waterer.py')
parent_parser.add_argument('--seed', type=int, default=int(time.time()), help='Random seed used by many different things, but especially when reshuffling sequences between partitioning steps, and during simulation. Set this if you want to get exactly the same result when rerunning.')
parent_parser.add_argument('--min-observations-per-gene', type=int, default=20, help='If a gene is observed fewer times than this, we average over other alleles/primary versions/etc. (e.g. in recombinator and hmmwriter). Also used as a more general "this isn\'t very many observations" value.')
parent_parser.add_argument('--no-per-base-mfreqs', action='store_true', help='When making the HMM model files, instead of the default of accounting for different rates to different bases (i.e. A->T vs A->G), do *not* account for the different rates to different bases. This is only really useful for testing the new simulation option --per-base-mutation.')
parent_parser.add_argument('--region-end-exclusion-length', type=int, default=0, help='when counting/writing parameters, ignore this many bases abutting non-templated insertions for calculating mutation frequencies (note: doesn\'t make a difference (hence set to 0 by default) probably because we\'re setting a strongish prior on these bases when writing hmms anyway')
parent_parser.add_argument('--allow-conserved-codon-deletion', action='store_true', help='When building hmm yaml model files during parameter caching, allow deletions that extend through the conserved codons (cyst in V and tryp/phen in J) (by default such deletions are forbidden; see https://github.com/psathyrella/partis/issues/308). NOTE that this has *no* effect if you\'ve already cached parameters.')
parent_parser.add_argument('--subcluster-annotation-size', default=3, help='when running the bcrham viterbi algorithm, instead of running the multi-hmm on the entire family, instead split the family into subclusters of (about) this size, then replace each subcluster with its naive sequence and iterate until running on once cluster of (about) this size consisting entirely of inferred naive/ancestor sequences (see https://github.com/psathyrella/partis/issues/308). Set to the string \'None\' to turn off.')

parent_parser.add_argument('--only-genes', help='Colon-separated list of genes to which to restrict the analysis. If any regions (V/D/J) are not represented among these genes, these regions are left unrestricted. If running \'simulate\', you probably also want to set --force-dont-generate-germline-set.')
parent_parser.add_argument('--n-max-per-region', default='3:5:2', help='Number of best smith-waterman matches (per region, in the format v:d:j) to pass on to the hmm')
parent_parser.add_argument('--gap-open-penalty', type=int, default=30, help='Penalty for indel creation in Smith-Waterman step.')
parent_parser.add_argument('--max-vj-mut-freq', type=float, default=0.4, help='skip sequences whose mutation rates in V and J are greater than this (it\'s really not possible to get meaningful smith-waterman matches above this)')
parent_parser.add_argument('--max-logprob-drop', type=float, default=5., help='stop glomerating when the total logprob has dropped by this much')
parent_parser.add_argument('--n-simultaneous-seqs', type=int, help='Number of simultaneous sequences on which to run the multi-HMM (e.g. 2 for a pair hmm)')
parent_parser.add_argument('--all-seqs-simultaneous', action='store_true', help='Run all input sequences simultaneously, i.e. equivalent to setting --n-simultaneous-seqs to the number of input sequences.')
parent_parser.add_argument('--simultaneous-true-clonal-seqs', action='store_true', help='If action is annotate/cache-parameters, run true clonal sequences together simultaneously with the multi-HMM. If actions is partition, skip clustering entirely and instead use the true partition (useful for e.g. validating selection metrics, where you don\'t want to be conflating partition performance with selection metric performance).')
parent_parser.add_argument('--mimic-data-read-length', action='store_true', help='In simulation, trim V 5\' and D 3\' to mimic read lengths seen in data (must also be set when caching parameters)')

parent_parser.add_argument('--infname', help='input sequence file in .fa, .fq, .csv, or partis output .yaml (if .csv, specify id string and sequence headers with --name-column and --seq-column)')
parent_parser.add_argument('--paired-indir', help='Directory with input files for use with --paired-loci. Must conform to file naming conventions from bin/split-loci.py (really paircluster.paired_dir_fnames()), i.e. the files generated when --infname and --paired-loci are set.')
parent_parser.add_argument('--name-column', help='column/key name for sequence ids in input csv/yaml file (default: \'unique_ids\'). If set to \'fasta-info-index-N\' for an integer N, it will take the Nth (zero-indexed) value from a fasta files uid line.')
parent_parser.add_argument('--seq-column', help='column/key name for nucleotide sequences in input csv/yaml file (default: \'input_seqs\')')
parent_parser.add_argument('--input-metafname', help='yaml file with meta information for the sequences in --infname (and --queries-to-include-fname), keyed by sequence id. If running multiple steps (e.g. cache-parameters and partition), this must be set for all steps. Currently accepted keys/columns are \'timepoint\', \'affinity\', \'subject\' and \'multiplicity\'. See https://github.com/psathyrella/partis/blob/master/docs/subcommands.md#input-meta-info for an example.')
parent_parser.add_argument('--input-partition-fname', help='partis-style json/yaml file with a partition to use during annotation, i.e. annotate the sequences in --infname using the partition in this file, rather than the default of annotating each sequence individually. Used in \'merge-paired-partitions\' when we want to annotate a list of sequences according to a new, joint partition.')
parent_parser.add_argument('--input-partition-index', type=int, help='Index of the partition to be read from --input-partition-fname (if unset, defaults to the best partition). To figure out which index you want, you probably want to run the view-output action on the file.')
parent_parser.add_argument('--outfname', help='output file name')
parent_parser.add_argument('--paired-outdir', help='Directory for all output files when --paired-loci is set, i.e. involving multiple loci in input and/or paired heavy/light information.')
parent_parser.add_argument('--write-full-yaml-output', action='store_true', help='By default, we write yaml output files using the json subset of yaml, since it\'s much faster. If this is set, we instead write full yaml, which is more human-readable (but also much slower).')
parent_parser.add_argument('--presto-output', action='store_true', help='Write output file(s) in presto/changeo format. Since this format depends on a particular IMGT alignment, this depends on a fasta file with imgt-gapped alignments for all the V, D, and J germline genes. The default in data/germlines/<species>/imgt-alignments/, is probably fine for most cases. For the \'annotate\' action, a single .tsv file is written with annotations (so --outfname suffix must be .tsv). For the \'partition\' action, a fasta file is written with cluster information (so --outfname suffix must be .fa or .fasta), as well as a .tsv in the same directory with the corresponding annotations.')
parent_parser.add_argument('--airr-output', action='store_true', help='Write output file(s) in AIRR-C format (if --outfname has suffix .tsv, only the airr .tsv is written; however if --outfname has suffix .yaml, both the standard partis .yaml file and an airr .tsv are written). A description of the airr columns can be found here https://docs.airr-community.org/en/stable/datarep/rearrangements.html#fields.')
parent_parser.add_argument('--airr-input', action='store_true', help='Read --infname in airr format. Equivalent to setting \'--seq-column sequence --name-column sequence_id\'.')
parent_parser.add_argument('--extra-annotation-columns', help='Extra columns to add to the (fairly minimal) set of information written by default to annotation output files (choose from: %s)' % ' '.join(utils.extra_annotation_headers))  # NOTE '-columns' in command line arg, but '-headers' in utils (it's more consistent that way, I swear)
parent_parser.add_argument('--cluster-annotation-fname', help='output file for cluster annotations (default is <--outfname>-cluster-annotations.<suffix>)')
parent_parser.add_argument('--parameter-dir', help='Directory to/from which to write/read sample-specific parameters. If not specified, a default location is used (and printed to std out). If it does not exist, we infer parameters before proceeding to the desired action.')
parent_parser.add_argument('--parameter-type', default=None, choices=utils.parameter_type_choices, help='Use parameters from Smith-Waterman (sw), single-sequence HMM (hmm), or multi-sequence HMM (multi-hmm) subdirectories for inference and simulation? \'multi-hmm\' is best, but requires running \'partition\' with --count-parameters set; \'hmm\' is much better than \'sw\', but \'sw\' is occasionally useful for debugging. If not set, looks first for \'multi-hmm\', then \'hmm\', then \'sw\', if none are found defaults to %s.'%utils.default_parameter_type)
parent_parser.add_argument('--parameter-out-dir', help='Special parameter dir for writing multi-hmm parameters, i.e. when running annotate or partition with --count-parameters set (if not set, defaults to <--parameter-dir>/multi-hmm).')
parent_parser.add_argument('--refuse-to-cache-parameters', action='store_true', help='Disables auto parameter caching, i.e. if --parameter-dir doesn\'t exist, instead of inferring parameters, raise an exception. Useful for batch/production use where you want to make sure you\'re caching parameters in a separate step.')
parent_parser.add_argument('--persistent-cachefname', help='Name of file which will be used as an initial cache file (if it exists), and to which all cached info will be written out before exiting.')
parent_parser.add_argument('--sw-cachefname', help='Smith-Waterman cache file name. Default is set using a hash of all the input sequence ids (in partitiondriver, since we have to read the input file first).')
parent_parser.add_argument('--write-sw-cachefile', action='store_true', help='Write sw results to the sw cache file during actions for which we\'d normally only look for an existing one (i.e annotate and partition).')
parent_parser.add_argument('--workdir', help='Temporary working directory (default is set below)')

parent_parser.add_argument('--plotdir', help='Base directory to which to write plots (by default this is not set, and consequently no plots are written. If --paired-loci is set, you can set --plotdir to \'paired-outdir\' to copy over the value from that argument.')
parent_parser.add_argument('--plot-annotation-performance', action='store_true', help='Compare inferred and true annotation values (deletion lengths, gene calls, etc.). If --plotdir is set, write corresponding plots to disk, and see also --print-n-worst-annotations.')
parent_parser.add_argument('--plot-partitions', action='store_true', help='after running annotation, plot partitions on resulting cluster annotations. This only makes sense if partitions were specified with --input-partition-fname, and only with \'annotate\' and \'merge-paired-partitions\' actions. Can also use \'plot-partitions\' action')
parent_parser.add_argument('--no-mds-plots', action='store_true', help='don\'t make mds plots when plotting partitions (they\'re slowish)')
parent_parser.add_argument('--print-n-worst-annotations', type=int, help='For use with --plot-annotation-performance: print ascii annotations for the N least accurate annotations according to several annotation values (e.g. VD insertion length, distance to true naive sequence). One of two ways to visualize annotation performance -- the other is by setting --plotdir to look at summary plots over all families. View with "less -RS".')
parent_parser.add_argument('--no-partition-plots', action='store_true', help='don\'t make paritition plots, even if --plotdir is set (presumably because you want other plots, e.g. for selection metrics -- the partition plots have a lot more depenencies)')
parent_parser.add_argument('--only-csv-plots', action='store_true', help='skip writing actual image files, which can quite be slow, and only write the csv/yaml summaries (where implemented)')
parent_parser.add_argument('--make-per-gene-plots', action='store_true', help='in addition to plots aggregating over genes, write plots displaying info for each gene of, e.g., per position shm rate, deletion frequencies')
parent_parser.add_argument('--make-per-gene-per-base-plots', action='store_true', help='in addition to the plots made by --make-per-gene-plots, also make the per-gene, per-base plots (i.e. showing A->T vs A->G (this is quite slow, like a few seconds per gene plot).')
parent_parser.add_argument('--ete-path', default=('/home/%s/anaconda_ete/bin' % os.getenv('USER')) if os.getenv('USER') is not None else None, help='Set to the string \'None\' to turn off.')

parent_parser.add_argument('--default-initial-germline-dir', default=partis_dir + '/data/germlines', help='For internal use only (used to pass around the default location). To specify your own germline directory from which to start, use --initial-germline-dir instead.')
parent_parser.add_argument('--initial-germline-dir', help='Directory with fastas from which to read germline set. Only used when caching parameters, during which its contents is copied into --parameter-dir, usually (i.e. unless germline inference is turned off) with modification. NOTE default is set below, because control flow is too complicated for argparse')
parent_parser.add_argument('--sanitize-input-germlines', action='store_true', help='By default we require all gene names to look like IGHVx*y. If you set this option when you\'re also setting --initial-germline-dir, then we instead allow arbitrary strings as gene names in this input germline directory, and we sanitize them by adding the correct locus and region info at the start (so make sure that you have --locus set correctly), and adding an arbitrary allele string at the end (e.g. <stuff> --> IGHV<stuff>*x).')
parent_parser.add_argument('--simulation-germline-dir', help='Germline directory that was used for simulation (used if --is-simu is set, altough if the default germline info is similar to that used for simulation it may not be necessary)')
parent_parser.add_argument('--aligned-germline-fname', help='fasta file with imgt-gapped alignments for each V, D, and J gene (used by --presto-output). The defaults are in data/germlines/<species>/imgt-alignments/ (set in processargs.py). The existing alignments are automatically extended with any missing genes, but no guarantees are made as to the similarity of this to what imgt would do.')
parent_parser.add_argument('--dtr-path', help='path to directory with decision tree regression model files')  # , default=partis_dir + '/data/selection-metrics/dtr-models'

parent_parser.add_argument('--n-max-alleles-per-gene', type=int, default=None, help='if set, after allele inference the germline set is reduced such that each imgt-name-defined gene has at most this many alleles, with the alleles assigned to the fewest sequences removed first.')
parent_parser.add_argument('--typical-genes-per-region-per-subject', default='55:25:6', help='typical number of alleles per subject for the v, d, and j regions (used by --min-allele-prevalence-fraction)')
parent_parser.add_argument('--min-allele-prevalence-fraction', type=float, default=0.0005, help='Remove any V alleles that represent less than this fraction of the repertoire (rescaled using --typical-genes-per-region-per-subject for D and J). Converted to --min-allele-prevalence-fractions (note plural!) internally.')

parent_parser.add_argument('--leave-default-germline', action='store_true')
parent_parser.add_argument('--dont-remove-unlikely-alleles', action='store_true')
parent_parser.add_argument('--allele-cluster', action='store_true')
parent_parser.add_argument('--kmeans-allele-cluster', action='store_true')
parent_parser.add_argument('--dont-find-new-alleles', action='store_true')
# parent_parser.add_argument('--always-find-new-alleles', action='store_true', help='By default we only look for new alleles if a repertoire\'s mutation rate is amenable to reasonable new-allele sensitivity (i.e. if it\'s not crazy high). This overrides that.')
parent_parser.add_argument('--debug-allele-finding', action='store_true', help='print lots of debug info on new-allele fits')
parent_parser.add_argument('--new-allele-fname', help='fasta fname to which to write any new alleles (they are also written, together with previously-known alleles that are also in the sample, to --parameter-dir)')
parent_parser.add_argument('--n-max-snps', type=int, default=8, help='when new-allele finding, look for new V alleles separated from existing V alleles by up to this many SNPs. Also used for allele removal (corresponding numbers for D and J are set automatically)')
parent_parser.add_argument('--n-max-mutations-per-segment', type=int, default=23, help='when new-allele finding, exclude sequences which have more than this many mutations in the V segment')
parent_parser.add_argument('--min-allele-finding-gene-length', type=int, default=150, help='if (after excluding particularly short reads) the reads for a gene are shorter than this, then don\'t look for new alleles with/on this gene')
parent_parser.add_argument('--plot-and-fit-absolutely-everything', type=int, help='fit every single position for this <istart> and write every single corresponding plot (slow as hell, and only for debugging/making plots for paper)')
parent_parser.add_argument('--cluster-indices', help='indices of clusters (when sorted largest to smallest) to print for the \'view-output\' action. Specified as a colon-separated list, where each item can be either a single integer or a python slice-style range of integers, e.g. 0:3-6:50 --> 0:3:4:5:50')
parent_parser.add_argument('--min-selection-metric-cluster-size', type=int, default=treeutils.default_min_selection_metric_cluster_size, help='don\'t calculate selection metrics for clusters smaller than this')
parent_parser.add_argument('--treefname', help='newick-formatted file with a tree corresponding to the sequences either in --infname (if making new output, i.e. action is annotate or partition) or --outfname (if reading existing output, i.e. action is get-selection-metrics).')
parent_parser.add_argument('--selection-metric-fname', help='yaml file to which to write selection metrics. If not set, and if --add-metrics-to-outfname is not set, this defaults to <--outfname>.replace(<suffix>, \'-selection-metrics\' + <suffix>)')
parent_parser.add_argument('--add-selection-metrics-to-outfname', action='store_true', help='If set, instead of writing a separate file with selection metrics, we include them in <--outfname> under the key \'tree-info\'.')
parent_parser.add_argument('--lb-tau', type=float, default=treeutils.default_lb_tau, help='exponential decay length for local branching index (lbi). See also --lbr-tau-factor.')  # default is set in treeutils so we can import it in bin/get-tree-metrics.py UPDATE deleted that script, so could probably move this
parent_parser.add_argument('--lbr-tau-factor', type=int, default=treeutils.default_lbr_tau_factor, help='factor by which to multiply --lb-tau in order to get the exponential decay length (tau) for local branching ratio (lbr). The ratio (lbr) should, in general, have tau much larger than for the index (lbi). See also --lb-tau.')  # default is set in treeutils so we can import it in bin/get-selection-metrics.py UPDATE deleted that script, so could probably move this
parent_parser.add_argument('--dont-normalize-lbi', action='store_true', help='By default, we normalize tau so that 0. is a rough minimum, and 1. is a fairly large value. But since you can\'t normalize for tau larger than 1/seq_len, it\'s useful to turn off normalization.')
parent_parser.add_argument('--include-relative-affy-plots', action='store_true', help='in addition to validation plots using actual affinity from simulation, also make plots using \'relative\' affinity (see bcr-phylo), i.e. a cell\'s affinity relative only to other cells alive at the same point in time. Useful because the selection metrics attempt to predict the effects of fitness on branchiness only at a given time (or: if you sample a lot of intermediate ancestors, your selection metric performance will look artificially bad because the ancestors, while having higher affinity than other cells alive at the same time, have affinity lower than the leaves).')

parent_parser.add_argument('--no-sw-vsearch', action='store_true', help='By default, we get preliminary V annotations from vsearch that we pass to sw. This improves accuracy because the vsearch shm rate estimates let us separate sequences into groups by optimal sw match/mismatch scores, and improves speed by letting us reverse shm V indels before running sw (so fewer sw iterations; also vsearch is faster than sw). Setting this option skips the preliminary vsearch step (although if caching parameters, we still use vsearch to remove less-likely genes; but no vsearch info is passed to sw)')

# ----------------------------------------------------------------------------------------
subconfig = collections.OrderedDict((
    ('version'          , {'func' : int, 'help' : 'print version information and exit'}),  # int doesn't do anything, it's just because I have to put something here
    ('cache-parameters' , {'func' : run_partitiondriver, 'help' : 'Cache parameter values and write hmm model files.'}),
    ('annotate'         , {'func' : run_partitiondriver, 'help' : 'Annotate sequences in input file, i.e. run the viterbi algorithm, using pre-existing parameter directory.'}),
    ('partition'        , {'func' : run_partitiondriver, 'help' : 'Partition sequences in input file into clonally-related families using pre-existing parameter directory.'}),
    ('simulate'         , {'func' : run_simulation,      'help' : 'Generate simulated sequences based on information in pre-existing parameter directory.'}),
    ('view-output'      , {'func' : run_partitiondriver, 'help' : 'Print partitions and/or annotations from an existing output yaml file.'}),
    ('merge-paired-partitions', {'func' : run_partitiondriver, 'help' : 'Merge the heavy and light chain partition paths from two existing output files.'}),
    ('view-alternative-annotations' , {'func' : run_partitiondriver, 'help' : 'Print (to std out) a comparison of the naive sequences and v/d/j gene calls corresponding to sub- and super-clusters of the cluster specified with --queries. You must have specified --calculate-alternative-naive-seqs in a previous partition step so that this information was saved.'}),
    ('plot-partitions'  , {'func' : run_partitiondriver, 'help' : 'Plot existing partitions and cluster annotations.'}),
    ('get-selection-metrics' , {'func' : run_partitiondriver, 'help' : 'Calculate selection metrics using existing output.'}),
    ('get-linearham-info', {'func' : run_partitiondriver, 'help' : 'Write input file for linearham (to --linearham-info-fname), using a previous partis output (--outfname) file as input.'}),
    # deprecated actions:
    ('view-annotations' , {'func' : run_partitiondriver, 'help' : 'Mostly deprecated: Print annotations from an existing old-style annotation output csv (for current yaml output files, use \'view-output\').'}),
    ('view-partitions'  , {'func' : run_partitiondriver, 'help' : 'Mostly deprecated: Print partitions from an existing old-style partition output csv (for current yaml output files, use \'view-output\').'}),
    ('view-alternative-naive-seqs'  , {'func' : run_partitiondriver, 'help' : 'DEPRECATED use \'view-alternative-annotations\''}),
    ('run-viterbi'      , {'func' : run_partitiondriver, 'help' : 'DEPRECATED use \'annotate\''}),
))

def runs_on_existing_output(actionstr):
    return actionstr.split('-')[0] in ['view', 'plot', 'get', 'merge']

subargs = {subname : [] for subname in subconfig}

# ----------------------------------------------------------------------------------------
subargs['annotate'].append({'name' : '--get-selection-metrics', 'kwargs' : {'action' : 'store_true', 'help' : 'calculate tree-based selection metrics for each cluster.'}})

# ----------------------------------------------------------------------------------------
subargs['partition'].append({'name' : '--naive-hamming', 'kwargs' : {'action' : 'store_true', 'help' : 'agglomerate purely with naive hamming distance, i.e. set the low and high preclustering bounds to the same value'}})
subargs['partition'].append({'name' : '--naive-vsearch', 'kwargs' : {'action' : 'store_true', 'help' : 'Very fast clustering: infer naive (unmutated ancestor) for each input sequence, then toss it all into vsearch. But, of course, not as accurate as the slower methods.'}})
subargs['partition'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'Throw out all sequences that are not clonally related to this sequence id. Much much much faster than partitioning the entire sample (well, unless your whole sample is one family).'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['partition'].append({'name' : '--seed-seq', 'kwargs' : {'help' : 'same effect as --seed-unique-id, but specifies the sequence instead of that sequence\'s id (so that it doesn\'t have to be in the original input file)'}})
subargs['partition'].append({'name' : '--random-seed-seq', 'kwargs' : {'action' : 'store_true', 'help' : 'choose a sequence at random from the input file, and use it as the seed for seed partitioning (as if it had been set as the --seed-unique-id)'}})
subargs['partition'].append({'name' : '--annotation-clustering', 'kwargs' : {'help' : 'Perform annotation-based clustering: group together sequences with the same V and J, same CDR3 length, and 90%% cdr identity. Very, very inaccurate.'}})
subargs['partition'].append({'name' : '--annotation-clustering-thresholds', 'kwargs' : {'default' : '0.9', 'help' : 'colon-separated list of thresholds for annotation-based clustering (i.e. a cluster is defined as the same v and j gene plus this threshold on cdr3 nucleotide hamming distance)'}})
subargs['partition'].append({'name' : '--naive-hamming-bounds', 'kwargs' : {'help' : 'Clustering bounds (lo:hi colon-separated pair) on naive sequence hamming distance. If not specified, the bounds are set based on the per-dataset mutation levels. For most purposes should be left at the defaults.'}})
subargs['partition'].append({'name' : '--logprob-ratio-threshold', 'kwargs' : {'type' : float, 'default' : 18., 'help' : 'reaches a min value of <this> minus five for large clusters.'}})
subargs['partition'].append({'name' : '--synthetic-distance-based-partition', 'kwargs' : {'action' : 'store_true', 'help' : 'Use simulation truth info to create a synthetic distance-based partition (for validation).'}})
subargs['partition'].append({'name' : '--cache-naive-hfracs', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to naive sequences and log probabilities, also cache naive hamming fractions between cluster pairs. Only really useful for plotting or testing.'}})
subargs['partition'].append({'name' : '--n-precache-procs', 'kwargs' : {'type' : int, 'help' : 'Number of processes to use when precaching naive sequences. Default is set based on some heuristics, and should typically only be overridden for testing.'}})
subargs['partition'].append({'name' : '--biggest-naive-seq-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--biggest-logprob-cluster-to-calculate', 'kwargs' : {'type' : int, 'default' : 15, 'help' : 'start thinking about subsampling before you calculate anything if cluster is bigger than this'}})
subargs['partition'].append({'name' : '--n-partitions-to-write', 'kwargs' : {'type' : int, 'default' : 10, 'help' : 'Number of partitions (surrounding the best partition) to write to output file.'}})
subargs['partition'].append({'name' : '--naive-swarm', 'kwargs' : {'action' : 'store_true', 'help' : 'Use swarm instead of vsearch, which the developer recommends. Didn\'t seem to help much, and needs more work to optimize threshold, so DO NOT USE.'}})
subargs['partition'].append({'name' : '--small-clusters-to-ignore', 'kwargs' : {'help' : 'colon-separated list (or dash-separated inclusive range) of cluster sizes to throw out after several partition steps. E.g. \'1:2\' will, after <--n-steps-at-which-to-ignore-small-clusters> partition steps, throw out all singletons and pairs. Alternatively, \'1-10\' will ignore all clusters with size less than 11.'}})
subargs['partition'].append({'name' : '--n-steps-after-which-to-ignore-small-clusters', 'kwargs' : {'type' : int, 'default' : 3, 'help' : 'number of partition steps after which to throw out small clusters (where "small" is controlled by <--small-clusters-to-ignore>). (They\'re thrown out before this if we get to n_procs one before this).'}})
subargs['partition'].append({'name' : '--n-final-clusters', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and there are still more than this many clusters, attempt to keep merging until there aren\'t.  If --min-largest-cluster-size is also set, we stop if either of their criteria are satisfied'}})
subargs['partition'].append({'name' : '--min-largest-cluster-size', 'kwargs' : {'type' : int, 'help' : 'If you reach the maximum likelihood partition and the largest cluster isn\'t this big, attempt to keep merging until it is. If --n-final-clusters is also set, we stop if either of their criteria are satisfied.'}})
subargs['partition'].append({'name' : '--calculate-alternative-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'write to disk all the information necessary to, in a later step (\'view-alternative-annotations\'), print alternative inferred naive sequences (i.e. visualize uncertainty in the inferred naive sequence). This is largely equivalent to setting --write-additional-cluster-annotations to \'sys.max_int:sys.max_int\'.'}})
subargs['partition'].append({'name' : '--max-cluster-size', 'kwargs' : {'type' : int, 'help' : 'stop clustering immediately if any cluster grows larger than this (useful for limiting memory usage, which can become a problem when the final partition contains very large clusters)'}})
subargs['partition'].append({'name' : '--write-additional-cluster-annotations', 'kwargs' : {'help' : 'in addition to writing annotations for each cluster in the best partition, also write annotations for all the clusters in several partitions on either side of the best partition. Specified as a pair of numbers \'m:n\' for m partitions before, and n partitions after, the best partition.'}})
subargs['partition'].append({'name' : '--get-selection-metrics', 'kwargs' : {'action' : 'store_true', 'help' : 'calculate tree-based selection metrics for each cluster (can also be run on existing output with \'get-selection-metrics\' action). By default it calculates fast, but very approximate, trees (see manual for details), but you can give it more accurate trees by setting --treefname (unrelated to --input-simulation-treefname).'}})

# ----------------------------------------------------------------------------------------
# basic simulation:
subargs['simulate'].append({'name' : '--mutation-multiplier', 'kwargs' : {'type' : float, 'help' : 'Multiply observed branch lengths by some factor when simulating, e.g. if in data it was 0.05, but you want closer to ten percent in your simulation, set this to 2'}})
subargs['simulate'].append({'name' : '--n-sim-events', 'kwargs' : {'type' : int, 'default' : 1, 'help' : 'Number of rearrangement events to simulate'}})
subargs['simulate'].append({'name' : '--n-trees', 'kwargs' : {'type' : int, 'help' : 'Number of phylogenetic trees from which to choose during simulation (we pre-generate this many trees before starting a simulation run, then for each rearrangement event choose one at random -- so this should be at least of order the number of simulated events, so your clonal families don\'t all have the same tree).'}})
subargs['simulate'].append({'name' : '--n-leaf-distribution', 'kwargs' : {'default' : None, 'choices' : ['geometric', 'box', 'zipf', 'hist'], 'help' : 'Distribution from which to draw the number of leaves for each tree. Except for \'hist\' (which is a histogram inferred from data) the distribution\'s parameter comes from --n-leaves. If not set, defaults to value given by --default-scratch-n-leaf-distribution if rearranging from scratch, but \'hist\' if instead rearranging with inferred parameters. Note that if the hist file doesn\'t exist, it\'ll fall back to one of the others (with warning).'}})
subargs['simulate'].append({'name' : '--default-scratch-n-leaf-distribution', 'kwargs' : {'default' : 'geometric', 'choices' : ['geometric', 'box', 'zipf'], 'help' : 'Default for --n-leaf-distribution. Do not set, for internal use only.'}})
subargs['simulate'].append({'name' : '--n-leaves', 'kwargs' : {'type' : float, 'default' : 5., 'help' : 'Parameter determining the shape of the distribution fromff which the number of leaves per tree is drawn (--n-leaf-distribution). For \'geometric\' and \'box\', this is the mean; while for \'zipf\' it is the exponent, i.e. *not* the mean -- see docs for numpy.random.zipf for details. Has *no* effect if --n-leaf-distribution is \'hist\'.'}})
subargs['simulate'].append({'name' : '--constant-number-of-leaves', 'kwargs' : {'action' : 'store_true', 'help' : 'Give all trees the same number of leaves'}})
subargs['simulate'].append({'name' : '--allowed-cdr3-lengths', 'kwargs' : {'help' : 'Colon-separated list of cdr3 lengths to which to restrict the simulation. NOTE that our cdr3 definition includes both conserved codons, which differs from the imgt definition (sorry).'}})
subargs['simulate'].append({'name' : '--remove-nonfunctional-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'Remove non-functional sequences from simulated rearrangement events. Note that because this happens after generating SHM (since we have no way to tell bppseqgen to only generate functional sequences), you will in general need to specify a (potentialy much) larger value for --n-leaves if you set --remove-nonfunctional-seqs. Typically, the vast majority of nonfunctional simulated sequences are due to stop codons generated by SHM.'}})
subargs['simulate'].append({'name' : '--gtrfname', 'kwargs' : {'default' : partis_dir + '/data/recombinator/gtr.txt', 'help' : 'File with list of GTR parameters. Fed into bppseqgen along with the chosen tree. Corresponds to an arbitrary dataset at the moment, but eventually will be inferred per-dataset.'}})  # NOTE command to generate gtr parameter file: [stoat] partis/ > zcat /shared/silo_researcher/Matsen_F/MatsenGrp/data/bcr/output_sw/A/04-A-M_gtr_tr-qi-gi.json.gz | jq .independentParameters | grep -v '[{}]' | sed 's/["\:,]//g' | sed 's/^[ ][ ]*//' | sed 's/ /,/' | sort >data/gtr.txt)
subargs['simulate'].append({'name' : '--root-mrca-weibull-parameter', 'kwargs' : {'type' : float, 'help' : 'if set, uses TreeSimGM (instead of TreeSim), and passes this value as the weibull parameter (e.g. 0.1: long root-mrca distance, lots of shared mutation; 5: short, little) NOTE requires installation of TreeSimGM'}})
subargs['simulate'].append({'name' : '--input-simulation-treefname', 'kwargs' : {'help' : 'file with newick-formatted lines corresponding to trees to use for simulation. Note that a) the tree depths are rescaled according to the shm rates requested by other command line arguments, i.e. the depths in the tree file are ignored, and b) the resulting sequences do not use the leaf names from the trees (unrelated to --treefname).'}})
subargs['simulate'].append({'name' : '--generate-trees', 'kwargs' : {'action' : 'store_true', 'help' : 'Run the initial tree-generation step of simulation (writing to --outfname), without then proceeding to actually generate the sequences. Used for paired heavy/light simulation so we can pass the same list of trees to both.'}})
subargs['simulate'].append({'name' : '--choose-trees-in-order', 'kwargs' : {'action' : 'store_true', 'help' : 'Instead of the default of choosing a tree at random from the list of trees (which is either generated at the start of the simulation run, or passed in with --input-simulation-treefname), instead choose trees sequentially. If there\'s more events than trees, it cycles through the list again. Used for paired heavy/light simulation.'}})
subargs['simulate'].append({'name' : '--check-tree-depths', 'kwargs' : {'action' : 'store_true', 'help' : 'check how close the fraction of mutated bases that we get from bppseqgen is to the depth of the trees that we passed in.'}})
subargs['simulate'].append({'name' : '--no-per-base-mutation', 'kwargs' : {'action' : 'store_true', 'help' : 'By default the simulation uses both a different SHM rate for each position in each gene, and also a different rate to each base for each of these positions (e.g. position 37 in IGHV1-2*02 might mutate 3x as much as neighboring bases, and it might mutate to T twice as often as G). This option turns off the per-base part of that, so while each position in each gene still has a different overall rate, the rate to each base is the same (e.g. A->T is the same as A->G). Run time with this option set is about 10 times faster than the default. NOTE also --no-per-base-mfreqs'}})
subargs['simulate'].append({'name' : '--mutate-conserved-codons', 'kwargs' : {'action' : 'store_true', 'help' : 'By default we don\'t let mutations occur in conserved codons (cyst in V and tryp/phen in J), but if this option is set we let them mutate.'}})
subargs['simulate'].append({'name' : '--light-chain-fractions', 'kwargs' : {'default' : 'igk,0.67:igl,0.33', 'help' : 'fraction of events with igk vs igl in paired simulation, in form \'igk,f1:igl,f2\', where f1+f2 must equal to 1.'}})
# shm indels:
subargs['simulate'].append({'name' : '--indel-frequency', 'kwargs' : {'default' : 0., 'type' : float, 'help' : 'fraction of simulated sequences with SHM indels'}})
subargs['simulate'].append({'name' : '--n-indels-per-indeld-seq', 'kwargs' : {'default' : '1:2', 'help' : 'list of integers from which to choose the number of SHM indels in each sequence that we\'ve already decided has indels'}})
subargs['simulate'].append({'name' : '--mean-indel-length', 'kwargs' : {'type' : float, 'default' : 5, 'help' : 'mean length of each SHM indel (geometric distribution)'}})
subargs['simulate'].append({'name' : '--indel-location', 'kwargs' : {'help' : 'Where to put simulated SHM indels. Set either to a single integer position at which all indels will occur, or choose from one of three regions (each excluding the first and last five positions in the sequence): None (anywhere in sequence), \'v\' (before cysteine), \'cdr3\' (within cdr3). If set to a single position, any entries in --n-indels-per-indeld-seq greater than 1 are removed, since we don\'t want a bunch of them at the same point.'}})
# from-scratch (rather than mimicking a particular data sample):
subargs['simulate'].append({'name' : '--rearrange-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for rearrangement-level parameters, and instead make up some plausible stuff from scratch. Have to also set --shm-parameter-dir.'}})
subargs['simulate'].append({'name' : '--mutate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'Don\'t use an existing parameter directory for shm-level (mutation) parameters, and instead make up stuff from scratch (by default this means shm rate varies over positions and sequences, but is the same for all regions). Have to also set --reco-parameter-dir.'}})
subargs['simulate'].append({'name' : '--simulate-from-scratch', 'kwargs' : {'action' : 'store_true', 'help' : 'same as setting both --rearrange-from-scratch and --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--shm-parameter-dir', 'kwargs' : {'help' : 'parameter directory from which to retrieve shm-level info when --rearrange-from-scratch is set (to set germline info, use --initial-germline-dir).'}})
subargs['simulate'].append({'name' : '--reco-parameter-dir', 'kwargs' : {'help' : 'parameter directory from which to retrieve rearrangement-level info when --mutate-from-scratch is set (to set germline info, use --initial-germline-dir).'}})
subargs['simulate'].append({'name' : '--scratch-mute-freq', 'kwargs' : {'type' : float, 'default' : 0.05, 'help' : 'shm rate used by --mutate-from-scratch'}})
subargs['simulate'].append({'name' : '--flat-mute-freq', 'kwargs' : {'action' : 'store_true', 'help' : 'use the same shm rate (--scratch-mute-freq) for all positions (in practice it\'s not that much flatter than the Gamma that is used by default --mutate-from-scratch). For use with --mutate-from-scratch.'}})
subargs['simulate'].append({'name' : '--same-mute-freq-for-all-seqs', 'kwargs' : {'action' : 'store_true', 'help' : 'use the same shm rate (--scratch-mute-freq) for all sequences. For use with --mutate-from-scratch. NOTE: this means we tell bppseqgen to use the same expected rate for every sequence -- there\'s still variance in the resulting number of output mutation per sequence.'}})
# new allele/germline set generation (this is also for from-scratch)
subargs['simulate'].append({'name' : '--generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Choose a realistic germline set from the available genes. Turned on automatically when --rearrange-from-scratch or --simulate-from-scratch are set.'}})
subargs['simulate'].append({'name' : '--force-dont-generate-germline-set', 'kwargs' : {'action' : 'store_true', 'help' : 'Force --generate-germline-set to False in situations in which it would otherwise be turned on (I know, this is kind of messy, but I think it\'s the best available solution).'}})
subargs['simulate'].append({'name' : '--n-genes-per-region', 'kwargs' : {'help' : 'colon-separated list specifying the number of genes (not alleles -- i.e. the *total* number of alleles is this times the number of alleles per gene) for each region (for use with --generate-germline-set). Default (set outside argparse, so argparse incorrectly thinks it\'s None): %s' % glutils.default_n_genes_per_region}})
subargs['simulate'].append({'name' : '--n-sim-alleles-per-gene', 'kwargs' : {'help' : 'colon-separated list of mean alleles per gene for each region (for use with --generate-germline-set). Default (set outside argparse, so argparse incorrectly thinks it\'s None): %s' % glutils.default_n_alleles_per_gene}})
subargs['simulate'].append({'name' : '--min-sim-allele-prevalence-freq', 'kwargs' : {'default' : glutils.default_min_allele_prevalence_freq, 'type' : float, 'help' : 'minimum frequency at which alleles are allowed to occur, e.g. if it\'s 0.01 then each pair of V alleles will have a prevalence ratio between 0.01 and 1'}})
subargs['simulate'].append({'name' : '--allele-prevalence-fname', 'kwargs' : {'help' : 'abandon help all ye who enter here'}})
subargs['simulate'].append({'name' : '--nsnp-list', 'kwargs' : {'help' : 'When --generate-germline-set is set, this is a colon-separated list whose length gives the number of novel alleles to add to the germline set. Each entry in the list is the number of SNPs to generate for the corresponding new allele. If both --nsnp-list and --nindel-list are set, they must be the same length; if one is unset, it is assumed to be all zeroes.'}})
subargs['simulate'].append({'name' : '--nindel-list', 'kwargs' : {'help' : 'When --generate-germline-set is set, this is a colon-separated list whose length gives the number of novel alleles to add to the germline set. Each entry in the list is the number of indels to generate for the corresponding new allele. If both --nsnp-list and --nindel-list are set, they must be the same length; if one is unset, it is assumed to be all zeroes.'}})
subargs['simulate'].append({'name' : '--im-a-subproc', 'kwargs' : {'action' : 'store_true', 'help' : 'Internal use only. This is set to true if this is a subprocess, i.e. set when --n-procs > 1.'}})
subargs['simulate'].append({'name' : '--mean-cells-per-droplet', 'kwargs' : {'type' : float, 'help' : 'After simulating each family as either h+k or h+l (and adding the corresponding, correct \'paired-uids\' key to the output files, proceed to mimic a 10x experiment with multiple cells in some droplets, and writing the corresponding (incorrect) \'paired-uids\' to a new input meta file (which, when passed with --input-metafname, will override the paired uids from the regular simulation file)'}})


subargs['view-alternative-annotations'].append({'name' : '--print-all-annotations', 'kwargs' : {'action' : 'store_true', 'help' : 'In addition to printing each alternative naive seq and gene call, with information on which clusters support its correctness, also print the annotations for all of these clusters below each naive sequence and gene call.'}})
subargs['view-alternative-annotations'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['view-output'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['view-output'].append({'name' : '--extra-print-key', 'kwargs' : {'help' : 'add the value of this key to the ascii art annotation'}})
subargs['get-selection-metrics'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)
subargs['plot-partitions'].append({'name' : '--seed-unique-id', 'kwargs' : {'help' : 'see help for this option under \'partition\' action'}})  # NOTE do *not* move these up above -- we forbid people to set them for auto parameter caching (see exception above)

subargs['get-linearham-info'].append({'name' : '--linearham-info-fname', 'kwargs' : {'help' : 'yaml file to which to write linearhmam input information'}})

def get_arg_names(actions):  # return set of all arg names (in the form they appear in args.__dict__) for the specified actions
    if actions == 'all':
        actions = subconfig.keys()
    return set([actionconf['name'][2:].replace('-', '_') for action in actions for actionconf in subargs[action]])

# above we just made a dict with lists of args, here we actually make the sub parsers
subparsermap = {}
for name, vals in subconfig.items():
    subparsermap[name] = subparsers.add_parser(name, parents=[parent_parser], help=vals['help'], formatter_class=MultiplyInheritedFormatter)
    subparsermap[name].set_defaults(func=vals['func'])  # set the default fcn to run for <name> (i.e. action)
    for argconf in subargs[name]:
        subparsermap[name].add_argument(argconf['name'], **argconf['kwargs'])

# ----------------------------------------------------------------------------------------
args = parser.parse_args()

# add OR of all arguments to all subparsers to <args>, as None (to avoid having to rewrite a *##!(%ton of other code)
for missing_arg in get_arg_names('all') - set(args.__dict__):  # NOTE see also remove_action_specific_args() above, which kind of does similar things, at least if i rewrote this all from scratch only one (or neither...) would exist
    args.__dict__[missing_arg] = None

if args.outfname is None and args.paired_outdir is None and runs_on_existing_output(args.action):  # would be better to have this in processargs.py like everything else, but then I'd have to import the function somehow
    raise Exception('--outfname (or --paired-outdir, if using --paired-loci) required for %s' % args.action)

processargs.process(args)
random.seed(args.seed)
numpy.random.seed(args.seed)
start = time.time()
args.func(args)
print '      total time: %.1f' % (time.time()-start)
